{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1337ade9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# Taste-wise Greedy Forward Selection (CV-based, no test-leak)  [RESUMABLE]\n",
    "# - Outer: 7:3 stratified Train/Test (stratify=y_class 유지)\n",
    "# - Inner: KFold (비층화, 카테고리 미사용)  ✅ 요청 반영\n",
    "# - 체크포인트 저장/재개 + (옵션) k-step 단위 저장\n",
    "# =========================================================\n",
    "\n",
    "# =========================================================\n",
    "# 0) Imports\n",
    "# =========================================================\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold          # ★ StratifiedKFold -> KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "# =========================================================\n",
    "# 1) PREP: Build X, Y, y_class (if missing)\n",
    "# =========================================================\n",
    "target_path = \"/home/a202152010/flavor\"\n",
    "excel_filename = \"Supplemental Files and Figure source files.xlsx\"\n",
    "\n",
    "try:\n",
    "    os.chdir(target_path)\n",
    "    print(f\"작업 경로 변경: {os.getcwd()}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"[WARN] 경로를 찾을 수 없습니다: {target_path}\")\n",
    "\n",
    "try:\n",
    "    from init_modeling import generate_X\n",
    "    print(\"[OK] imported generate_X from init_modeling.py\")\n",
    "except ModuleNotFoundError:\n",
    "    print(\"[WARN] init_modeling 모듈을 찾지 못했습니다. fallback generate_X를 사용합니다.\")\n",
    "\n",
    "    def generate_X(df: pd.DataFrame, impute: bool = True):\n",
    "        X_df = df.copy()\n",
    "        if \"beer_id\" in X_df.columns:\n",
    "            X_df = X_df.drop(columns=[\"beer_id\"])\n",
    "        X_df = X_df.apply(pd.to_numeric, errors=\"coerce\")\n",
    "        if impute:\n",
    "            imp = SimpleImputer(strategy=\"median\")\n",
    "            X_np = imp.fit_transform(X_df.values)\n",
    "        else:\n",
    "            X_np = X_df.values\n",
    "        return X_np.astype(np.float32)\n",
    "\n",
    "if not all([v in globals() for v in [\"X\", \"Y\", \"y_class\"]]):\n",
    "    chem_dataset = pd.read_excel(excel_filename, sheet_name=\"Supplementary File S1\").set_index(\"beer\")\n",
    "    trained_panel_dataset = pd.read_excel(excel_filename, sheet_name=\"Supplementary File S4\").set_index(\"beer\")\n",
    "    chem_dataset_expertpanel = chem_dataset.join(trained_panel_dataset, lsuffix=\"_chem\", rsuffix=\"_panel\")\n",
    "\n",
    "    beer_id_col = chem_dataset_expertpanel[\"beer_id_chem\"].rename(\"beer_id\")\n",
    "    target_features = chem_dataset_expertpanel.loc[:, \"acetaldehyde\":\"sulfur_sum\"]\n",
    "\n",
    "    X_input = pd.concat([beer_id_col, target_features], axis=1)\n",
    "    X = generate_X(X_input, impute=True)  # (N,231)\n",
    "\n",
    "    Y = chem_dataset_expertpanel.loc[:, \"A_malt_all\":\"overall\"]  # (N,50)\n",
    "    y_class = chem_dataset_expertpanel.loc[:, \"tasting_category_fine_chem\"]  # (N,)\n",
    "\n",
    "    feature_names = list(target_features.columns)\n",
    "    target_names  = list(Y.columns)\n",
    "\n",
    "    print(\"[PREP] built X, Y, y_class\")\n",
    "    print(\"X shape:\", np.array(X).shape)\n",
    "    print(\"Y shape:\", Y.shape)\n",
    "else:\n",
    "    print(\"[PREP] X, Y, y_class already exist -> skip build\")\n",
    "\n",
    "# =========================================================\n",
    "# 2) Config\n",
    "# =========================================================\n",
    "OUT_DIR  = \"Model_performance\"\n",
    "PLOT_DIR = os.path.join(OUT_DIR, \"taste_plots\")\n",
    "CKPT_DIR = os.path.join(OUT_DIR, \"_checkpoint\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "os.makedirs(PLOT_DIR, exist_ok=True)\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "\n",
    "# ---- Resumable options ----\n",
    "RESUME_IF_EXISTS         = True\n",
    "SAVE_PER_K_STEP          = True\n",
    "SAVE_EVERY_K             = 1\n",
    "\n",
    "# ---- Outer split ----\n",
    "OUTER_TEST_SIZE = 0.30\n",
    "OUTER_SEED      = 0\n",
    "\n",
    "# ---- Inner CV (KFold: 비층화) ----\n",
    "INNER_SEED      = 42\n",
    "INNER_N_SPLITS  = 5          # ★ 자유롭게 선택 가능(3~5 권장)             # *\n",
    "\n",
    "# ---- Greedy ----\n",
    "K_MAX           = 231\n",
    "PATIENCE        = 8\n",
    "MIN_IMPROVE     = 1e-4\n",
    "\n",
    "# ---- Overfit control ----\n",
    "USE_GAP_PENALTY = True\n",
    "LAMBDA_GAP      = 0.8\n",
    "\n",
    "# ---- MLP ----\n",
    "ALPHAS          = [1e-3, 1e-2, 1e-1]\n",
    "HIDDEN          = (64, 64)\n",
    "MAX_ITER        = 2000\n",
    "\n",
    "# =========================================================\n",
    "# 3) Names\n",
    "# =========================================================\n",
    "X_np = np.asarray(X)\n",
    "Y_df = Y if isinstance(Y, pd.DataFrame) else pd.DataFrame(Y)\n",
    "y_cls = pd.Series(y_class).reset_index(drop=True).values  # ★ Outer stratify에만 사용\n",
    "\n",
    "N, n_features = X_np.shape\n",
    "n_targets = Y_df.shape[1]\n",
    "\n",
    "if \"feature_names\" in globals() and isinstance(feature_names, (list, tuple)) and len(feature_names) == n_features:\n",
    "    feat_list = list(feature_names)\n",
    "else:\n",
    "    feat_list = [f\"chem_{i:03d}\" for i in range(n_features)]\n",
    "\n",
    "if \"target_names\" in globals() and isinstance(target_names, (list, tuple)) and len(target_names) == n_targets:\n",
    "    taste_list = list(target_names)\n",
    "else:\n",
    "    taste_list = list(Y_df.columns)\n",
    "\n",
    "# =========================================================\n",
    "# 4) Outer split + scaling  (여기만 stratify 유지)\n",
    "# =========================================================\n",
    "try:\n",
    "    X_tr_raw, X_te_raw, Y_tr_raw, Y_te_raw, y_tr_cls, y_te_cls = train_test_split(\n",
    "        X_np, Y_df.values, y_cls,\n",
    "        test_size=OUTER_TEST_SIZE,\n",
    "        random_state=OUTER_SEED,\n",
    "        shuffle=True,\n",
    "        stratify=y_cls\n",
    "    )\n",
    "except ValueError:\n",
    "    print(\"[WARN] Outer stratified split failed -> random split used.\")\n",
    "    X_tr_raw, X_te_raw, Y_tr_raw, Y_te_raw, y_tr_cls, y_te_cls = train_test_split(\n",
    "        X_np, Y_df.values, y_cls,\n",
    "        test_size=OUTER_TEST_SIZE,\n",
    "        random_state=OUTER_SEED,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "sx = StandardScaler().fit(X_tr_raw)\n",
    "sy = StandardScaler().fit(Y_tr_raw)\n",
    "\n",
    "X_tr = sx.transform(X_tr_raw).astype(np.float32)\n",
    "X_te = sx.transform(X_te_raw).astype(np.float32)\n",
    "Y_tr = sy.transform(Y_tr_raw).astype(np.float32)\n",
    "Y_te = sy.transform(Y_te_raw).astype(np.float32)\n",
    "\n",
    "print(f\"[Outer] Train={X_tr.shape[0]}, Test={X_te.shape[0]} | X={X_tr.shape[1]}, Y={Y_tr.shape[1]}\")\n",
    "print(\"[Inner CV] Using KFold (NON-stratified) -> n_splits =\", INNER_N_SPLITS)\n",
    "\n",
    "# ★ Inner CV splitter: KFold (yc ls 사용 안 함)\n",
    "kf = KFold(n_splits=INNER_N_SPLITS, shuffle=True, random_state=INNER_SEED)\n",
    "\n",
    "# =========================================================\n",
    "# 5) Utilities\n",
    "# =========================================================\n",
    "def safe_r2(y_true, y_pred):\n",
    "    if not np.isfinite(y_pred).all():\n",
    "        return np.nan\n",
    "    if np.std(y_true) < 1e-8:\n",
    "        return np.nan\n",
    "    return r2_score(y_true, y_pred)\n",
    "\n",
    "def make_mlp(alpha, seed):\n",
    "    return MLPRegressor(\n",
    "        hidden_layer_sizes=HIDDEN,\n",
    "        activation=\"relu\",\n",
    "        solver=\"adam\",\n",
    "        alpha=alpha,\n",
    "        learning_rate_init=1e-3,\n",
    "        max_iter=MAX_ITER,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.15,\n",
    "        n_iter_no_change=20,\n",
    "        tol=1e-5,\n",
    "        random_state=seed,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "# ★ ycls 인자 제거: KFold는 y 필요 없음\n",
    "def cv_eval_cols(X, y, cols, base_seed):\n",
    "    best = (-np.inf, np.inf, None)  # mean, se, alpha\n",
    "    for alpha in ALPHAS:\n",
    "        r2_list = []\n",
    "        for fold_i, (tr_i, va_i) in enumerate(kf.split(X)):\n",
    "            seed = int(base_seed + 1000 * fold_i)  # cand 무관 고정\n",
    "            model = make_mlp(alpha=alpha, seed=seed)\n",
    "            model.fit(X[tr_i][:, cols], y[tr_i])\n",
    "            pred = model.predict(X[va_i][:, cols])\n",
    "            r2v = safe_r2(y[va_i], pred)\n",
    "            if np.isnan(r2v):\n",
    "                r2v = -np.inf\n",
    "            r2_list.append(r2v)\n",
    "\n",
    "        r2_arr  = np.array(r2_list, dtype=float)\n",
    "        mean_r2 = float(np.mean(r2_arr))\n",
    "        se_r2   = float(np.std(r2_arr, ddof=1) / np.sqrt(len(r2_arr))) if len(r2_arr) > 1 else 0.0\n",
    "\n",
    "        if mean_r2 > best[0]:\n",
    "            best = (mean_r2, se_r2, alpha)\n",
    "    return best\n",
    "\n",
    "def refit_train_test_r2(Xtr, ytr, Xte, yte, cols, alpha, seed):\n",
    "    model = make_mlp(alpha=alpha, seed=seed)\n",
    "    model.fit(Xtr[:, cols], ytr)\n",
    "    pred_tr = model.predict(Xtr[:, cols])\n",
    "    pred_te = model.predict(Xte[:, cols])\n",
    "    return safe_r2(ytr, pred_tr), safe_r2(yte, pred_te)\n",
    "\n",
    "def score_with_gap(val_mean, train_r2):\n",
    "    if not USE_GAP_PENALTY:\n",
    "        return float(val_mean)\n",
    "    if (not np.isfinite(val_mean)) or (not np.isfinite(train_r2)):\n",
    "        return float(val_mean)\n",
    "    gap = float(train_r2 - val_mean)\n",
    "    return float(val_mean - LAMBDA_GAP * max(0.0, gap))\n",
    "\n",
    "# =========================================================\n",
    "# 6) Checkpoint I/O helpers\n",
    "# =========================================================\n",
    "def _taste_safe_name(taste: str) -> str:\n",
    "    return taste.replace(\"/\", \"_\").replace(\"\\\\\", \"_\").replace(\":\", \"_\")\n",
    "\n",
    "def ckpt_paths(taste: str):\n",
    "    t = _taste_safe_name(taste)\n",
    "    return {\n",
    "        \"summary\": os.path.join(CKPT_DIR, f\"summary__{t}.csv\"),\n",
    "        \"progress\": os.path.join(CKPT_DIR, f\"progress__{t}.csv\"),\n",
    "        \"doneflag\": os.path.join(CKPT_DIR, f\"DONE__{t}.flag\"),\n",
    "    }\n",
    "\n",
    "def is_done(taste: str) -> bool:\n",
    "    return os.path.exists(ckpt_paths(taste)[\"doneflag\"])\n",
    "\n",
    "def save_ckpt(taste: str, summary_row: dict, prog_df: pd.DataFrame, done: bool):\n",
    "    p = ckpt_paths(taste)\n",
    "    pd.DataFrame([summary_row]).to_csv(p[\"summary\"], index=False, encoding=\"utf-8-sig\")\n",
    "    prog_df.to_csv(p[\"progress\"], index=False, encoding=\"utf-8-sig\")\n",
    "    if done:\n",
    "        with open(p[\"doneflag\"], \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"done\")\n",
    "\n",
    "def load_ckpt(taste: str):\n",
    "    p = ckpt_paths(taste)\n",
    "    if not os.path.exists(p[\"summary\"]) or not os.path.exists(p[\"progress\"]):\n",
    "        return None, None, False\n",
    "    summ = pd.read_csv(p[\"summary\"])\n",
    "    prog = pd.read_csv(p[\"progress\"])\n",
    "    done = os.path.exists(p[\"doneflag\"])\n",
    "    return summ, prog, done\n",
    "\n",
    "# =========================================================\n",
    "# 7) Per-taste greedy forward (with periodic checkpoint)\n",
    "# =========================================================\n",
    "def run_one_taste(taste_idx: int):\n",
    "    taste = taste_list[taste_idx]\n",
    "    ytr = Y_tr[:, taste_idx].astype(np.float32)\n",
    "    yte = Y_te[:, taste_idx].astype(np.float32)\n",
    "    taste_seed = 10000 + taste_idx * 100\n",
    "\n",
    "    # ------------------------------\n",
    "    # (A) k=1 univariate best by CV\n",
    "    # ------------------------------\n",
    "    best_start = None\n",
    "    best_pack = None\n",
    "\n",
    "    for fi in range(n_features):\n",
    "        val_mean, val_se, alpha = cv_eval_cols(X_tr, ytr, [fi], base_seed=taste_seed + 10)\n",
    "        tr_r2, te_r2 = refit_train_test_r2(X_tr, ytr, X_te, yte, [fi], alpha=alpha, seed=taste_seed + 999)\n",
    "        sc = score_with_gap(val_mean, tr_r2)\n",
    "\n",
    "        if best_start is None or sc > best_pack[-1]:\n",
    "            best_start = fi\n",
    "            best_pack = (val_mean, val_se, alpha, tr_r2, te_r2, sc)\n",
    "\n",
    "    selected = [int(best_start)]\n",
    "    steps = []\n",
    "\n",
    "    def push_step(k, added_idx, val_mean, val_se, alpha, tr_r2, te_r2, sc):\n",
    "        prev = steps[-1] if len(steps) else None\n",
    "        d_test = (te_r2 - prev[\"test_r2\"]) if (prev is not None and np.isfinite(te_r2) and np.isfinite(prev[\"test_r2\"])) else np.nan\n",
    "        d_val  = (val_mean - prev[\"val_r2\"]) if (prev is not None and np.isfinite(val_mean) and np.isfinite(prev[\"val_r2\"])) else np.nan\n",
    "\n",
    "        steps.append({\n",
    "            \"taste\": taste,\n",
    "            \"k\": int(k),\n",
    "            \"added_feature\": feat_list[int(added_idx)],\n",
    "            \"added_feat_idx\": int(added_idx),\n",
    "\n",
    "            \"train_r2\": float(tr_r2) if np.isfinite(tr_r2) else np.nan,\n",
    "            \"val_r2\":   float(val_mean) if np.isfinite(val_mean) else np.nan,\n",
    "            \"val_se\":   float(val_se) if np.isfinite(val_se) else np.nan,\n",
    "            \"test_r2\":  float(te_r2) if np.isfinite(te_r2) else np.nan,\n",
    "\n",
    "            \"gap_train_minus_val\": float(tr_r2 - val_mean) if (np.isfinite(tr_r2) and np.isfinite(val_mean)) else np.nan,\n",
    "            \"score\": float(sc) if np.isfinite(sc) else np.nan,\n",
    "            \"alpha\": float(alpha),\n",
    "\n",
    "            \"delta_val\": float(d_val) if np.isfinite(d_val) else np.nan,\n",
    "            \"delta_test\": float(d_test) if np.isfinite(d_test) else np.nan,\n",
    "\n",
    "            \"selected_features_so_far\": \", \".join([feat_list[i] for i in selected]),\n",
    "        })\n",
    "\n",
    "    # k=1\n",
    "    val_mean, val_se, alpha, tr_r2, te_r2, sc = best_pack\n",
    "    push_step(1, best_start, val_mean, val_se, alpha, tr_r2, te_r2, sc)\n",
    "\n",
    "    best_score = steps[-1][\"score\"]\n",
    "    best_val_mean = steps[-1][\"val_r2\"]\n",
    "    best_val_se   = steps[-1][\"val_se\"]\n",
    "    best_k_by_score = 1\n",
    "    no_improve = 0\n",
    "\n",
    "    def build_partial_summary(stop_k, final_k_1se):\n",
    "        last = steps[-1]\n",
    "        return {\n",
    "            \"taste\": taste,\n",
    "            \"stop_k\": int(stop_k),\n",
    "            \"best_k_by_score\": int(best_k_by_score),\n",
    "            \"final_k_1se\": int(final_k_1se),\n",
    "\n",
    "            \"final_train_r2\": float(last[\"train_r2\"]),\n",
    "            \"final_val_r2\": float(last[\"val_r2\"]),\n",
    "            \"final_test_r2\": float(last[\"test_r2\"]),\n",
    "            \"final_gap\": float(last[\"gap_train_minus_val\"]) if np.isfinite(last[\"gap_train_minus_val\"]) else np.nan,\n",
    "            \"final_features\": last[\"selected_features_so_far\"],\n",
    "\n",
    "            \"full231_train_r2\": np.nan,\n",
    "            \"full231_val_r2\": np.nan,\n",
    "            \"full231_test_r2\": np.nan,\n",
    "        }\n",
    "\n",
    "    # k=2..K_MAX\n",
    "    for k in range(2, min(K_MAX, n_features) + 1):\n",
    "        remaining = [i for i in range(n_features) if i not in selected]\n",
    "\n",
    "        best_cand = None\n",
    "        best_pack = None\n",
    "\n",
    "        for cand in remaining:\n",
    "            cols = selected + [cand]\n",
    "            val_mean, val_se, alpha = cv_eval_cols(X_tr, ytr, cols, base_seed=taste_seed + 1000 * k)\n",
    "            tr_r2, te_r2 = refit_train_test_r2(X_tr, ytr, X_te, yte, cols, alpha=alpha, seed=taste_seed + 999 + k)\n",
    "            sc = score_with_gap(val_mean, tr_r2)\n",
    "\n",
    "            if best_cand is None or sc > best_pack[-1]:\n",
    "                best_cand = cand\n",
    "                best_pack = (val_mean, val_se, alpha, tr_r2, te_r2, sc)\n",
    "\n",
    "        selected.append(int(best_cand))\n",
    "        val_mean, val_se, alpha, tr_r2, te_r2, sc = best_pack\n",
    "        push_step(k, best_cand, val_mean, val_se, alpha, tr_r2, te_r2, sc)\n",
    "\n",
    "        if sc > best_score + MIN_IMPROVE:\n",
    "            best_score = sc\n",
    "            best_val_mean = val_mean\n",
    "            best_val_se   = val_se\n",
    "            best_k_by_score = k\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "\n",
    "        # ---- k-step 중간 저장 ----\n",
    "        if SAVE_PER_K_STEP and (k % SAVE_EVERY_K == 0):\n",
    "            prog_df = pd.DataFrame(steps)\n",
    "            thr = float(best_val_mean - best_val_se) if (np.isfinite(best_val_mean) and np.isfinite(best_val_se)) else float(best_val_mean)\n",
    "            k_1se_tmp = int(prog_df.loc[prog_df[\"val_r2\"] >= thr, \"k\"].min()) if np.isfinite(thr) else best_k_by_score\n",
    "            if not np.isfinite(k_1se_tmp):\n",
    "                k_1se_tmp = best_k_by_score\n",
    "            partial_summary = build_partial_summary(stop_k=prog_df[\"k\"].max(), final_k_1se=k_1se_tmp)\n",
    "            save_ckpt(taste, partial_summary, prog_df, done=False)\n",
    "\n",
    "        if no_improve >= PATIENCE:\n",
    "            break\n",
    "\n",
    "    prog = pd.DataFrame(steps)\n",
    "\n",
    "    # (C) 1-SE rule\n",
    "    thr = float(best_val_mean - best_val_se) if (np.isfinite(best_val_mean) and np.isfinite(best_val_se)) else float(best_val_mean)\n",
    "    k_1se = int(prog.loc[prog[\"val_r2\"] >= thr, \"k\"].min()) if np.isfinite(thr) else best_k_by_score\n",
    "    if not np.isfinite(k_1se):\n",
    "        k_1se = best_k_by_score\n",
    "\n",
    "    # (D) full-231\n",
    "    all_cols = list(range(n_features))\n",
    "    full_val_mean, full_val_se, full_alpha = cv_eval_cols(X_tr, ytr, all_cols, base_seed=taste_seed + 777777)\n",
    "    full_tr_r2, full_te_r2 = refit_train_test_r2(X_tr, ytr, X_te, yte, all_cols, alpha=full_alpha, seed=taste_seed + 888888)\n",
    "\n",
    "    row_final = prog[prog[\"k\"] == k_1se].iloc[0]\n",
    "    summary = {\n",
    "        \"taste\": taste,\n",
    "        \"stop_k\": int(prog[\"k\"].max()),\n",
    "        \"best_k_by_score\": int(best_k_by_score),\n",
    "        \"final_k_1se\": int(k_1se),\n",
    "\n",
    "        \"final_train_r2\": float(row_final[\"train_r2\"]),\n",
    "        \"final_val_r2\": float(row_final[\"val_r2\"]),\n",
    "        \"final_test_r2\": float(row_final[\"test_r2\"]),\n",
    "        \"final_gap\": float(row_final[\"gap_train_minus_val\"]) if pd.notna(row_final[\"gap_train_minus_val\"]) else np.nan,\n",
    "        \"final_features\": row_final[\"selected_features_so_far\"],\n",
    "\n",
    "        \"full231_train_r2\": float(full_tr_r2) if np.isfinite(full_tr_r2) else np.nan,\n",
    "        \"full231_val_r2\": float(full_val_mean) if np.isfinite(full_val_mean) else np.nan,\n",
    "        \"full231_test_r2\": float(full_te_r2) if np.isfinite(full_te_r2) else np.nan,\n",
    "    }\n",
    "\n",
    "    save_ckpt(taste, summary, prog, done=True)\n",
    "    return summary, prog\n",
    "\n",
    "# =========================================================\n",
    "# 8) Main loop (RESUME)\n",
    "# =========================================================\n",
    "all_summ_rows = []\n",
    "all_prog_dfs  = []\n",
    "\n",
    "for ti, taste in enumerate(taste_list):\n",
    "    if RESUME_IF_EXISTS and is_done(taste):\n",
    "        summ_df, prog_df, done = load_ckpt(taste)\n",
    "        print(f\"[SKIP] taste='{taste}' already done -> loaded checkpoint\")\n",
    "        all_summ_rows.append(summ_df.iloc[0].to_dict())\n",
    "        all_prog_dfs.append(prog_df)\n",
    "        continue\n",
    "\n",
    "    print(f\"[RUN] [{ti+1:02d}/{len(taste_list)}] taste='{taste}'\")\n",
    "    summ, prog = run_one_taste(ti)\n",
    "    all_summ_rows.append(summ)\n",
    "    all_prog_dfs.append(prog)\n",
    "\n",
    "summary_df = pd.DataFrame(all_summ_rows)\n",
    "progress_long = pd.concat(all_prog_dfs, ignore_index=True)\n",
    "\n",
    "# =========================================================\n",
    "# 9) Plot per taste\n",
    "# =========================================================\n",
    "def plot_taste_progress(taste_name: str):\n",
    "    df = progress_long[progress_long[\"taste\"] == taste_name].sort_values(\"k\")\n",
    "    if len(df) == 0:\n",
    "        return\n",
    "\n",
    "    summ = summary_df[summary_df[\"taste\"] == taste_name].iloc[0]\n",
    "    stop_k = int(summ[\"stop_k\"])\n",
    "    k1se   = int(summ[\"final_k_1se\"])\n",
    "    kbest  = int(summ[\"best_k_by_score\"])\n",
    "\n",
    "    x = df[\"k\"].values\n",
    "    y_val  = df[\"val_r2\"].values\n",
    "    y_test = df[\"test_r2\"].values\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(x, y_val,  label=\"val R2 (KFold mean)\")\n",
    "    plt.plot(x, y_test, label=\"test R2 (refit, log only)\")\n",
    "\n",
    "    plt.axvline(stop_k, linestyle=\"--\", linewidth=1.2, label=f\"stop_k={stop_k}\")\n",
    "    plt.axvline(kbest,  linestyle=\"--\", linewidth=1.2, label=f\"best_k={kbest}\")\n",
    "    plt.axvline(k1se,   linestyle=\"--\", linewidth=1.2, label=f\"k_1se={k1se}\")\n",
    "\n",
    "    plt.title(f\"{taste_name}: R2 vs k (Inner=KFold, Outer=stratified)\")\n",
    "    plt.xlabel(\"k (#features)\")\n",
    "    plt.ylabel(\"R2\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    out_png = os.path.join(PLOT_DIR, f\"{_taste_safe_name(taste_name)}.png\")\n",
    "    plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "for taste in taste_list:\n",
    "    plot_taste_progress(taste)\n",
    "\n",
    "print(f\"[OK] per-taste plots saved to: {PLOT_DIR}\")\n",
    "\n",
    "# =========================================================\n",
    "# 10) Save final Excel\n",
    "# =========================================================\n",
    "out_xlsx = os.path.join(OUT_DIR, \"tastewise_forward_selection__OuterStrat_InnerKFold__RESUMABLE.xlsx\")\n",
    "with pd.ExcelWriter(out_xlsx, engine=\"openpyxl\") as writer:\n",
    "    summary_df.to_excel(writer, sheet_name=\"summary\", index=False)\n",
    "    progress_long.to_excel(writer, sheet_name=\"progress_long\", index=False)\n",
    "\n",
    "print(f\"[OK] excel saved: {out_xlsx}\")\n",
    "\n",
    "print(\"\\n==================== SUMMARY (top by final_test_r2) ====================\")\n",
    "print(summary_df.sort_values(\"final_test_r2\", ascending=False)[[\n",
    "    \"taste\",\"final_k_1se\",\"final_val_r2\",\"final_train_r2\",\"final_test_r2\",\n",
    "    \"full231_val_r2\",\"full231_test_r2\"\n",
    "]].to_string(index=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
